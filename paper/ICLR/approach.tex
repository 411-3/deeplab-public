\section{Convolutional Neural Networks for Dense Image Labeling}
\label{sec:convnets}

\subsection{Model Architecture and Training}
\label{sec:convnet-arch}

Herein we describe how we have re-purposed the publicly available state-of-art
16 layer classification network of \cite{simonyan2014very} (VGG-16) into an efficient
and effective component of our sliding window detector.

\subsection{Efficient Dense Sliding Window Feature Extraction with the Hole Algorithm}
\label{sec:convnet-hole}

Dense spatial score evaluation is instrumental in the success of our CNN
sliding window detector. 

As a first step to implement this, we convert the fully-connected layers
of VGG-16 into convolutional ones and run the network in a convolutional
fashion on the patchwork. However this is not enough as it yields very
sparsely computed detection scores (with a stride of 32 pixels). To compute
scores more densely at our target stride of 8 pixels, we develop a variation
of the method previously employed by \cite{GCMG+13, sermanet2013overfeat}. We skip
subsampling after the last two max-pooling layers in the network of
\cite{simonyan2014very} and modify the convolutional filters in the layers that follow
them by introducing zeros to increase their length (\by{2}{} in the last three
convolutional layers and \by{4}{} in the first fully connected layer). We can
implement this more efficiently by keeping the filters intact and instead
sparsely sample the feature maps on which they are applied on using a stride
of 2 or 4 pixels, respectively. This approach is known as the `hole algorithm'
(`atrous algorithm') and has been developed before for efficient computation
of the undecimated wavelet transform \cite{Mall99}. We have implemented this
within the Caffe framework by adding to the \textsl{im2col} function (it
converts multi-channel feature maps to vectorized patches) the option to
sparsely sample the underlying feature map.

\subsection{Shrinking the Receptive Field of Pretrained Neural Networks}
\label{sec:convnet-field}

Explicit control on the receptive field size.

Most recent DCNN-based image recognition methods rely on networks pre-trained
on the Imagenet large-scale classification task. These networks typically have
large receptive field size, \by{224}{224} in the case of the VGG-16 net we
consider. We have found this receptive field size to be too large to allow
good localization accuracy (unless one uses heavily zoomed-in versions of the
image). Moreover, after converting the network to a fully convolutional one,
the first fully connected layer has 4,096 filters of large \by{7}{7} spatial
size and becomes the computational bottleneck in our sliding window detector
system.

We have addressed both of these serious practical problems by spatially
subsampling the first FC layer to \by{4}{4} spatial size. This has reduced the
receptive field of the network down to \by{128}{128} pixels and has reduced
computation time for the first FC layer by 3 times.


\section{Detailed Boundary Recovery: Fully-Connected Conditional Random Fields
  and Multi-scale Predictors}
\label{sec:boundary-recovery}

\subsection{Deep Convolutional Networks and the Localization Challenge}
\label{sec:local-chal}

As illustrated in Figure~\ref{fig:score-maps}, deep CNN feature maps can
reliably predict the presense and rough position of objects in an image but
are less well suited for pin-pointing their exact outline. There is a natural
trade-off between classification accuracy and localization accuracy with
convolutional networks: Deeper moders with multiple max-pooling layers have
proven most successful in classification tasks, however their increased
invariance and large receptive fields make the problem of inferring position
from the scores at their top output levels more challenging.

The reduced receptive field size in our network architecture can ameliorate
but not fully resolve this problem. Recent work has pursued two directions to
address this localization challenge. The first approach is to harness
information from multiple layers in the convolutional network in order to
better estimate the object boundaries. The second approach is
to employ a super-pixel representation, essentially delegating the
localization task to a low-level segmentation method. This route is followed
by the very successful recent method of \cite{mostajabi2014feedforward}.

In Section~\ref{sec:dense-crf}, we pursue a novel alternative direction based
on coupling the recognition capacity of deep CNNs and the fine-grained
localization accuracy of fully connected CRFs and show that it is remarkably
successful in addressing the localication challenge, producing accurate
semantic segmentation results and recovering object boundaries at a level of
detail that is well beyond the reach of existing methods.  We have also
pursued a hybrid approach in which we combine the proposed fully-connected CRF
method with our own variant of multi-scale prediction. This combined approach
yields a further performance improvement, as discussed in
Section~\ref{sec:multiscale}.

\subsection{Fully-Connected Conditional Random Fields for Accurate Localization}
\label{sec:dense-crf}

\begin{figure}[ht]
  \centering
  \begin{tabular}{c c}
  \end{tabular}
  \caption{(a) CNN classifier score map (dominant class). (b) Corresponding
    CRF belief map after convergence of the mean-field inference
    algorithm. \color{red}{Jay, is it possible to extract (b) from the
      inference code?}}
  \label{fig:score-maps}
\end{figure}

Traditionally, conditional random fields (CRFs) have been employed to smooth
noisy segmentation maps \cite{rother2004grabcut, kohli2009robust}. Typically
these models contain energy terms that couple neighboring nodes, favoring
same-label assignments to spatially proximal pixels. Qualitatively, the
primary function of these short-range CRFs has been to clean up the spurious
predictions of weak classifiers built on top of local hand-engineered features.

Compared to these weaker classifiers, modern deep CNN architectures such as
the one we use in this work produce feature maps and semantic label
predictions which are qualitatively different. As illustrated in
Figure~\ref{fig:score-maps}, the feature maps are typically quite smooth and
produce homogeneous classification results. In this regime, using short-range
CRFs can be detrimental, as our goal should be to recover detailed local
structure rather than further smooth it. Using contrast-sensitive potentials
\cite{rother2004grabcut} in conjunction to local-range CRFs can potentially
improve localization but still miss thin-structures and typically requires
solving an expensive discrete optimization problem.

In order to 

We briefly review the fully connected CRFs, and refer readers to
\citep{krahenbuhl2011efficient} for more details. We use the energy function
\begin{align}
  E(\boldsymbol{x}) = \sum_i \theta_i(x_i) + \sum_{ij} \theta_{ij}(x_i, x_j)
\end{align}
where $\boldsymbol{x}$ is the label assignment for pixels. We use as unary
potential $\theta_i(x_i) = - \log P(x_i)$, where $P(x_i)$ is the label
assignment probability at pixel $i$ as computed by DCNN. The pairwise
potential is $\theta_{ij}(x_i, x_j) = \sum_{m=1}^{K} w^m \cdot
k^m(\boldsymbol{f}_i, \boldsymbol{f}_j)$. There is one pairwise term for each
pair of pixels $i$ and $j$ in the image no matter how far from each other they
lie, \ie the model's factor graph is fully connected. Each $k^m$ is the
Gaussian kernel depends on features extracted for pixel $i$ and $j$ and is
weighted by parameter $w^m$. We adopt bilateral position and color terms,
specifically
\begin{align}
  \theta_{ij}(x_i, x_j) = w^1 \exp \Big(-\frac{|p_i-p_j|^2}{2\sigma_\alpha^2} -\frac{|I_i-I_j|^2}{2\sigma_\beta^2} \Big) + w^2 \exp \Big(-\frac{|p_i-p_j|^2}{2\sigma_\gamma^2}\Big)
\end{align}
where the first kernel depends on both pixel positions (denoted as $p$) and
pixel color intensities (denoted as $I$), and the second kernel only depends
on pixel positions. The hyper parameters $\sigma_\alpha$, $\sigma_\beta$ and
$\sigma_\gamma$ control the ``scale'' of the Gaussian kernels.

Crucially, this model is amenable to efficient approximate probabilistic
inference \citep{krahenbuhl2011efficient}. The message passing updates under a
fully decomposable mean field approximation $b(\boldsymbol{x}) = \prod_i
b_i(x_i)$ can be expressed as convolutions with a Gaussian kernel in feature
space. High-dimensional filtering algorihtms \citep{adams2010fast}
significantly speed-up this computation resulting in an algorithm that is very
fast in practice, less that 0.5 sec on average for Pascal VOC images using the
publicly available implementation of \citep{krahenbuhl2011efficient}.

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{fig/model_illustration.pdf}
  \caption{Model Illustration. The coarse output of Deep Convolutional Neural
    Network (with fully convolutional layers) is upsampled by bi-linear
    interpolation. A fully connected CRF is applied to refine the segmentation
    result.}
  \label{fig:ModelIllustration}
\end{figure}

\subsection{Multi-Scale Prediction}
\label{sec:multiscale}

GEORGE: Add description of the multi-scale net and also outline
  the fused system.

Following the recent success of ... we have explored a multi-scale variant of 
