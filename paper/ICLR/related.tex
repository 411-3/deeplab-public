\subsection{Related Work}
Our model is mostly related to two fields, and the goal of this work is to combine the best from them.

{\bf{Conditional Random Fields for segmentation: }} Many semantic segmentation methods rely on Conditional Random Fields (CRFs), which model the local interactions between pixels \citep{rother2004grabcut, shotton2009textonboost} or superpixels \citep{lucchi2011spatial} via the pairwise potential. Several works have been proposed to model the hierarchical dependency \citep{he2004multiscale, ladicky2009associative, lempitsky2011pylon} and the high-order potential (in addition to pairwise potential) \citep{delong2012fast, gonfaus2010harmony, kohli2009robust, krahenbuhl2011efficient}. Our model takes use of the efficient inference algorithm in fully connected CRF proposed by \citet{krahenbuhl2011efficient} for its powerful long range dependency.

{\bf{Deep Convolutional Neural Network for segmentation: }} Most of the systems built on top of DCNN classify either a single object label for an entire image \citep{KrizhevskyNIPS2013, simonyan2014very, szegedy2014going}, or several object labels for bounding boxes within an image \citep{papandreou2014untangling, girshick2014rcnn}. Recently, there are several works that attemp to semantically segment an image with DCNN. \citet{girshick2014rcnn, hariharan2014simultaneous} take both bounding box proposals and masked regions as input to DCNN. The masked regions provide extra object shape informantion to the neural networks. These methods heavily depend on the region proposals \citep{arbelaez2014multiscale, Uijlings13}. 
%Note that second order pooling \citep{carreira2012semantic} also assigns labels to regions proposals \citep{carreira2012cpmc}; however, hand-crafted features are employed. 
\citet{farabet2013learning} apply DCNN to multi-resolution of the input image and employ a segmentation tree to smooth the prediction results. We also notice that there are serveral concurrent works, which bear similarities to our proposed model. \citet{mostajabi2014feedforward} propose to combine the multi-scale cues extracted by DCNN (and some hand-crafted features) to label one superpixel. Similarly, their results depend on the superpixels, which may leak out the object boundaries, and the errors are difficult to recover. \citet{hariharan2014hypercolumns} propose to concatenate the computed inter-mediate feature maps within the DCNN for pixel classfication, and \citet{dai2014convolutional} propose to pool the inter-mediate feature maps by region proposals. Their models are two-step models, which depends on the accuracy of first step (\ie region proposals). On the other hand, our model is most similar to \citet{long2014fully, eigen2014predicting}, which directly apply DCNN to the whole image in sliding window fashion. The last fully connected layers within DCNN are replaced by convolutional layers. \citet{long2014fully} upsample and concatenate the scores from inter-mediate feature maps, while \citet{eigen2014predicting} refine the prediction result from coarse to fine by propagating the coarse results to another DCNN. 

{\bf{Combine Graphical Model and DCNN: }} The main difference between our model and other state-of-the-art models is the combination of graphical models and DCNN. Other works that are similar to ours include \citet{cogswell2014combining}, which also employ CRF to propose diverse region proposals \citep{yadollahpour2013discriminative} and extract features via DCNN, and \citet{chen2014learning} which efficiently blend inference and learning to jointly train DCNN and CRF. Our current model employs piecewise training, and jointly training is our next goal.



