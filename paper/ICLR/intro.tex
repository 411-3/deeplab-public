\section{Introduction}
\label{sec:intro}
%established in machine learning 
Deep Convolutional Neural Networks (DCNNs) had been the method of choice for document recognition since the work of \citet{LeCun1998}, but 
it is only recently that they have become the mainstream of high-level vison research.
Over the past two years  DCNNs have pushed the envelope of high-level vision system performance on several computer vision tasks, including image classifcation \citet{KrizhevskyNIPS2013, papandreou2014untangling, sermanet2013overfeat, simonyan2014very, szegedy2014going}, object detection \citet{girshick2014rcnn}, fine-graind categorization \citet{zhang2014part}, pose estimation \citet{chen2014articulated, tompson2014joint}, among others.
The most consistent results in these works is that DCNNs trained in an end-to-end manner  deliver  strikingly better results than systems relying on engineered features, such as SIFT or HOG features.

Still, the applications of DCNNs to low-level vision tasks, such as segmentation, or semantic image labelling  lags a bit behind in performance, with existing DCNN systems still underpeforming  state-of-the-art  computer vision systems using flat classifiers with engineered features, as witnessed e.g. in the comparisons of \cite{williams14} to the system of \cite{DollarZ13} on the task of boundary detection. 
 This can be partially attributed to the built-in  invariance of DCNNs to local image transformations, which underpins their ability to learn hierarchical abstractions of data \citep{zeiler2014visualizing}.
While this invariance is clearly desirable for high-level vision tasks, it can hamper low-level tasks, such as semantic segmentation - where we want precise localization, rather than abstraction of spatial details.  As such, DCNNs are not yet exploitable `out of the box' for dense image labelling tasks. 

There are two technical hurdles in the application of DCNNs to image labelling tasks: the first problem relates to the reduction of signal resolution incurred by the repeated combination of max-pooling and downsampling (`striding') performed at every layer of DCNNs.  Some of the most successful  DCNNs e.g. 
\citep{KrizhevskyNIPS2013, simonyan2014very, szegedy2014going}
effectively downsample an $N\times N$ image by a factor of $N$ to reach a 
a single  classification decision for the whole image. This is clearly not appropriate for the task of pixel-level labelling, where $N \times N$ labels are needed. This could  be naively alleaviated by upsampling the image by a factor of $N$, yielding an $N^2\times N^2$ image, which would then result in an $N\times N$ set of labels (ingoring boundary effects). Instead of this naive approach, as in  \cite{papandreou2014untangling}, we introduce into the problem the `atrous' (with holes) algorithm developped for  discrete wavelet analysis in  \cite{Mall99}, thereby achieving substantial gains in efficiency. 

The second problem relates to the fact that obtaining object-centric decisions from a classifier inherently requires invariance to spatial transformations - irrespective of signal downsampling. For instance, we cannot expect a `bicycle' classifier to respond differently to pixels that are  placed on the bicycle's support and to pixels that are placed in the interior of its holes, e.g. within the bicycle's chassis, or rays. In order to deal with this problem we turn to  Conditional Random Fields (CRFs) that have been broadly used in semantic segmentation to combine class scores computed by multi-way classifiers with the low-level information captured by the local interactions of pixels and edges\citep{rother2004grabcut, shotton2009textonboost} or superpixels \citep{lucchi2011spatial}. Even though works of increased sophistication have been proposed 
 to model the hierarchical dependency \citep{he2004multiscale, ladicky2009associative, lempitsky2011pylon} and/or  high-order dependencies of segments \citep{delong2012fast, gonfaus2010harmony, kohli2009robust, krahenbuhl2011efficient}, we use the  fully connected pairwise CRF proposed by \citet{krahenbuhl2011efficient} for its efficient computation, and powerful long range dependency. That model was shown in  \citet{krahenbuhl2011efficient} to largely improve the performance of a boosting-based pixel-level classifier, and in our work we demonstrate that it leads to state-of-the-art results when coupled with a DCNN-based pixel-level classifier. 

%invariance that is inherently necessary in order to obtain high-level, 
%In brief, we treat the first side of the problem by , and the second by coupling DCNNs with Conditional Random Field inference, so as to combine the high-level decisons of DCNNs with the sharp spatial localization of CRFs.

There  three main advantages of our method are (i) speed: by virtue of the `atrous' algorithm our DCNN classifier operates at XX fps, while Mean Field Inference for the fully-connected CRF requires XXXseconds, (ii) accuracy: we obtain state-of-the-art results on the PASCAL semantic segmentation challenge, outperforming the second-best approach of \citet{mostajabi2014feedforward} by a factor of XX$\%$ and (iii) simplicity: our system is composed of a cascade of two fairly well-established modules, DCNNs and CRFs, that can eventually be jointly trained. 

%In particular, when compared to a broad pool of works recently developped around the problem of combining DCNNs with semantic segmentation, our system can be understood as being substantially simpler, relying on a minimal number of components, while at the same time delivering state-of-the-art results.

%\citet{mostajabi2014feedforward} 

This is in contrast to the two-stage approaches that are now most common in semantic segmentation with DCNNs, using a cascade of bottom-up image segmentation and region classification. In particular, bounding box proposals and masked regions are used in 
\citet{girshick2014rcnn, hariharan2014simultaneous}  as inputs to a DCNN to introduce  shape information into the classification process; \citet{farabet2013learning} apply DCNNs at multiple image resolutions and employ a segmentation tree to smooth the prediction results;  \citet{mostajabi2014feedforward} label pre-computed superpixels by 
 combining  multi-scale cues extracted by DCNNs  some hand-crafted features;

%, but also  heavily depends on the region proposals \citep{arbelaez2014multiscale, Uijlings13};  a similar approach using manually engineered features is the second order pooling method of \citep{carreira2012semantic} that also assigns labels to regions proposals, delivered by \citep{carreira2012cpmc}. 

. We also notice that there are serveral concurrent works, which bear similarities to our proposed model.  However, they results depend on the superpixels, which may leak out the object boundaries. \citet{hariharan2014hypercolumns} propose to concatenate the computed inter-mediate feature maps within the DCNNs for pixel classfication, and \citet{dai2014convolutional} propose to pool the inter-mediate feature maps by region proposals. Their models are two-step models, which depends on the accuracy of first step (\ie region proposals). 


On the other hand, our model is most similar to \citet{long2014fully, eigen2014predicting}, which directly apply DCNNs to the whole image in a sliding window fashion, replacing the last fully connected layers within a DCNN  by convolutional layers (similar ideas have been pursued for classification in \cite{simonyan2014very,papandreou2014untangling}). \citet{long2014fully} upsample and concatenate the scores from inter-mediate feature maps, while \citet{eigen2014predicting} refine the prediction result from coarse to fine by propagating the coarse results to another DCNN. Another recent work that also attempts to combine the effectivenss of graphical model and DCNN for segmentation is \citet{cogswell2014combining}, where the authors employ CRFs to propose diverse region proposals, an extension of \citet{yadollahpour2013discriminative}.

%{\bf{Conditional Random Fields for segmentation
%,  or crop several  bounding boxes from an image \citep{, girshick2014rcnn} to deal with multiple objects.


%: %}} Many semantic segmentation methods rely on Conditional Random Fields (CRFs), which

%{\bf{Deep Convolutional Neural Network for segmentation: }} Most of the systems built on top of DCNNs classify either a single object label for an entire image \citep{KrizhevskyNIPS2013, simonyan2014very, szegedy2014going}, or several object labels for bounding boxes within an image \citep{papandreou2014untangling, girshick2014rcnn}. Recently, there are several works that attemp to semantically segment an image with DCNNs. 



