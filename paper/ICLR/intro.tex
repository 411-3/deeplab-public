\section{Introduction}
\label{sec:intro}
%established in machine learning 
 Deep Convolutional Neural Networks (DCNNs) have been the leading approach  at least
for document recognition since the  work of \citet{LeCun1998}, but 
it is only recently that their applications in  vision have become the mainstream of high-level vison research.
Over the past two years  DCNNs have 
 pushed the envelope of high-level vision system performance on several computer vision tasks, including image classifcation \citet{KrizhevskyNIPS2013, papandreou2014untangling, sermanet2013overfeat, simonyan2014very, szegedy2014going}, object detection \citet{girshick2014rcnn}, fine-graind categorization \citet{zhang2014part}, pose estimation \citet{chen2014articulated, tompson2014joint}, among others.
The most consistent results in these works is that DCNNs trained in an end-to-end manner  deliver  clearly better results than systems relying on engineered features, such as SIFT or HOG features.

Still, the applications of DCNNs to low-level vision tasks, such as segmentation, or semantic image labelling  lags a bit behind, with existing DCNN systems still underpeforming the state-of-the-art engineered computer vision systems, e.g. XXX, XXX.
 This can be partially attributed to the built-in  invariance of DCNNs to local image transformations, which underpins their ability to learn hierarchical abstraction of data \citep{zeiler2014visualizing}.
While this invariance is a blessing for high-level vision tasks, it can hamper low-level tasks, such as semantic segmentation - where we want precise localization, rather than abstraction of spatial details.  As such, DCNNs are not exploitable `out of the box' for labelling tasks. 

There two sides to the problem: the one relates to the down-sampling operations performed at every layer of DCNNs - used 


 with the intent of reducing the set of positions entertained in higher-level classification tasks. 
\subsection{Related Work}
Our model is mostly related to two fields, and the goal of this work is to combine the best from them.

{\bf{Conditional Random Fields for segmentation: }} Many semantic segmentation methods rely on Conditional Random Fields (CRFs), which model the local interactions (via the pairwise potential) between pixels \citep{rother2004grabcut, shotton2009textonboost} or superpixels \citep{lucchi2011spatial}. Several works have been proposed to model the hierarchical dependency \citep{he2004multiscale, ladicky2009associative, lempitsky2011pylon} and the high-order potential (not just pairwise potential) \citep{delong2012fast, gonfaus2010harmony, kohli2009robust, krahenbuhl2011efficient}. Our model takes use of the fully connected CRF proposed by \citet{krahenbuhl2011efficient} for its efficient computation, and powerful long range dependency.

{\bf{Deep Convolutional Neural Network for segmentation: }} Most of the systems built on top of DCNN classify either a single object label for an entire image \citep{KrizhevskyNIPS2013, simonyan2014very, szegedy2014going}, or several object labels for bounding boxes within an image \citep{papandreou2014untangling, girshick2014rcnn}. Recently, there are several works that attemp to semantically segment an image with DCNN. \citet{girshick2014rcnn, hariharan2014simultaneous} take both bounding box proposals and masked regions as input to DCNN. The masked regions provide extra object shape informantion to the neural networks. These methods heavily depend on the region proposals \citep{arbelaez2014multiscale, Uijlings13}. Note that second order pooling \citep{carreira2012semantic} also assigns labels to regions proposals \citep{carreira2012cpmc}; however, hand-crafted features are employed. \citet{farabet2013learning} apply DCNN to multi-resolution of the input image and employ a segmentation tree to smooth the prediction results. We also notice that there are serveral concurrent works, which bear similarities to our proposed model. \citet{mostajabi2014feedforward} propose to combine the multi-scale cues extracted by DCNN (and some hand-crafted features) to label one superpixel. However, they results depend on the superpixels, which may leak out the object boundaries. \citet{hariharan2014hypercolumns} propose to concatenate the computed inter-mediate feature maps within the DCNN for pixel classfication, and \citet{dai2014convolutional} propose to pool the inter-mediate feature maps by region proposals. Their models are two-step models, which depends on the accuracy of first step (\ie region proposals). On the other hand, our model is most similar to \citet{long2014fully, eigen2014predicting}, which directly apply DCNN to the whole image in sliding window. The last fully connected layers within DCNN are replaced by convolutional layers. \citet{long2014fully} upsample and concatenate the scores from inter-mediate feature maps, while \citet{eigen2014predicting} refine the prediction result from coarse to fine by propagating the coarse results to another DCNN. Another recent work that also attempts to combine the effectivenss of graphical model and DCNN for segmentation is \citet{cogswell2014combining}, where the authors employ CRFs to propose diverse region proposals, an extension of \citet{yadollahpour2013discriminative}.


