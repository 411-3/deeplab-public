\section{Introduction}
\label{sec:intro}
%established in machine learning 
Deep Convolutional Neural Networks (DCNNs) had been the method of choice for document recognition since  \citet{LeCun1998}, but 
have only recently become the mainstream of high-level vison research.
Over the past two years  DCNNs have pushed the performance of computer vision systems to soaring heights on a broad array of high-level problems, including image classifcation \citet{KrizhevskyNIPS2013, papandreou2014untangling, sermanet2013overfeat, simonyan2014very, szegedy2014going}, object detection \citet{girshick2014rcnn}, fine-graind categorization \citet{zhang2014part} and pose estimation \citet{chen2014articulated, tompson2014joint}, among others.
The most consistent result in these works is that DCNNs trained in an end-to-end manner  deliver  strikingly better results than systems relying on carefully engineered features, such as SIFT or HOG features.
This success can be partially attributed to the built-in  invariance of DCNNs to local image transformations, which underpins their ability to learn hierarchical abstractions of data \citep{zeiler2014visualizing}. While this invariance is clearly desirable for high-level vision tasks, it can hamper low-level tasks, such as semantic segmentation - where we want precise localization, rather than abstraction of spatial details.  As such, the DCNN architectures leading the game of high-level vision are not yet exploitable `out of the box' for low-level vision tasks, such as dense image labelling. 


%However, the applications of DCNNs to low-level vision tasks, such as segmentation, or semantic image labelling still lags a bit behind in performance, with existing DCNN systems underpeforming  systems that use flat classifiers with well-engineered features, as witnessed e.g. in boundary detection with DCNNs \cite{williams14} as opposed to the  Forest-based system of \cite{DollarZ13}. 

There are two technical hurdles in the application of DCNNs to image labelling tasks: signal downsampling, and spatial `insensitivity' (invariance). 
The first problem relates to the reduction of signal resolution incurred by the repeated combination of max-pooling and downsampling (`striding') performed at every layer of DCNNs.  Some of the most successful  DCNNs e.g. 
\citep{KrizhevskyNIPS2013, simonyan2014very, szegedy2014going}
effectively downsample an $N\times N$ image by a factor of $N$ to reach 
a single  classification decision for the whole image - while $N \times N$ labels are needed for pixel-level labelling. This could  be trivially alleaviated by upsampling the image by a factor of $N$, yielding an $N^2\times N^2$ image, which would then result in an $N\times N$ set of labels (ingoring boundary effects). Instead, as in  \cite{papandreou2014untangling}, we introduce into the problem the `atrous' (with holes) algorithm developped for  discrete wavelet analysis in  \cite{Mall99}, thereby achieving substantial gains in efficiency over this naive approach. 

The second problem relates to the fact that obtaining object-centric decisions from a classifier inherently requires invariance to spatial transformations. For instance, we cannot expect a `bicycle' classifier to respond differently to pixels that are  placed on the bicycle's support and to pixels that are placed in the interior of its holes, e.g. within the bicycle's chassis, or rays. In order to deal with this problem we turn to  Conditional Random Fields (CRFs) that have been broadly used in semantic segmentation to combine class scores computed by multi-way classifiers with the low-level information captured by the local interactions of   pixels and edges \citep{rother2004grabcut, shotton2009textonboost} or superpixels \citep{lucchi2011spatial}. Even though works of increased sophistication have been proposed 
 to model the hierarchical dependency \citep{he2004multiscale, ladicky2009associative, lempitsky2011pylon} and/or  high-order dependencies of segments \citep{delong2012fast, gonfaus2010harmony, kohli2009robust}, we use the  fully connected pairwise CRF proposed by \citet{krahenbuhl2011efficient} for its efficient computation, and ability to capture  long range dependencies. That model was shown in  \citet{krahenbuhl2011efficient} to largely improve the performance of a boosting-based pixel-level classifier, and in our work we demonstrate that it leads to state-of-the-art results when coupled with a DCNN-based pixel-level classifier. 

%invariance that is inherently necessary in order to obtain high-level, 
%In brief, we treat the first side of the problem by , and the second by coupling DCNNs with Conditional Random Field inference, so as to combine the high-level decisons of DCNNs with the sharp spatial localization of CRFs.

There  three main advantages of our method are (i) speed: by virtue of the `atrous' algorithm our DCNN classifier operates at XX fps, while Mean Field Inference for the fully-connected CRF requires XXXseconds, (ii) accuracy: we obtain state-of-the-art results on the PASCAL semantic segmentation challenge, outperforming the second-best approach of \citet{mostajabi2014feedforward} by a factor of 2$\%$ and (iii) simplicity: our system is composed of a cascade of two fairly well-established modules, DCNNs and CRFs, that can eventually be jointly trained with backpropagation. 

%In particular, when compared to a broad pool of works recently developped around the problem of combining DCNNs with semantic segmentation, our system can be understood as being substantially simpler, relying on a minimal number of components, while at the same time delivering state-of-the-art results.

%\citet{mostajabi2014feedforward} 

This is in contrast to the two-stage approaches that are now most common in semantic segmentation with DCNNs: such techniques typically use a cascade of bottom-up image segmentation and DCNN-based region classification, which makes the system commit to whatever errors of the front-end segmentation system.  
For instance, the bounding box proposals and masked regions delivered by \citep{arbelaez2014multiscale, Uijlings13} are used in 
\citet{girshick2014rcnn} and \cite{hariharan2014simultaneous}  as inputs to a DCNN to introduce  shape information into the classification process. Similarily, the authors of  \citet{mostajabi2014feedforward} label pre-computed superpixels using multi-scale DCNN cues extracted around them with some hand-crafted features, while a celebrated  non-DCNN precursor to these  works
is the second order pooling method of \citep{carreira2012semantic} which also assigns labels to the regions proposals delivered by \citep{carreira2012cpmc}. 
Understanding the perils of committing to a single segmentation, the authors of \citet{cogswell2014combining} 
build on \citep{yadollahpour2013discriminative} to explore a diverse set of CRF-based segmentation proposals, computed also by \citep{carreira2012cpmc}. These segmentation proposals are then re-ranked according to a DCNN trained in  particular for this reranking task. Even though this approach explicitly tries to handle the temperamental nature of a front-end segmentation algorithm, there is still no explicit exploitation of the DCNN scores in  the CRF-based segmentation algorithm: the DCNN is only applied post-hoc, while it would make sense to directly try to use its results {\em during} segmentation. 
% and the DCNN

Moving towards works that lie closer to our approach, several other researchers have considered the use of convolutionally computed DCNN features for dense image labelling -potentially coupled with some segmentation post-processing, rather than pre-processing. Among the first have been
\citet{farabet2013learning} who apply DCNNs at multiple image resolutions and then employ a segmentation tree to smooth the prediction results; more recently, \citet{hariharan2014hypercolumns} propose to concatenate the computed inter-mediate feature maps within the DCNNs for pixel classfication, and \citet{dai2014convolutional} propose to pool the inter-mediate feature maps by region proposals. Even though these works still employ  segmentation algorithms that are  decoupled from the DCNN classifier's results, we believe it is advantageous that segmentation is only used at a later stage, avoiding the commitment  to premature decisions. 

%, but also  heavily depends on the region proposals;  a similar approach using manually engineered features is 
%. We also notice that there are serveral concurrent works, which bear similarities to our proposed model.  However, they results depend on the superpixels, which may leak out the object boundaries.  Their models are two-step models, which depends on the accuracy of first step (\ie region proposals). 

More recently, the segmentation-free techniques of \citet{long2014fully} and \cite{eigen2014predicting} directly apply DCNNs to the whole image in a sliding window fashion, replacing the last fully connected layers of a DCNN  by convolutional layers. In order to deal with the spatial localization issues outlined in the beginning of the introduction, \citet{long2014fully} upsample and concatenate the scores from inter-mediate feature maps, while \citet{eigen2014predicting} refine the prediction result from coarse to fine by propagating the coarse results to another DCNN. 

%Another recent work that also attempts to combine the effectiveness of graphical model and DCNNs for segmentation is \citet{cogswell2014combining}, where the authors employ CRFs to propose diverse region proposals, an extension of \citet{yadollahpour2013discriminative}.

The main difference between our model and other state-of-the-art models is the combination of pixel-level CRFs and DCNN-based `unary terms'. Focusing on the closest    works in this direction, we remind  that  the authors of \citet{cogswell2014combining} use CRFs as a proposal mechanism for a DCNN-based reranking system, while the authors of  
\citet{farabet2013learning} treat superpixels as  nodes for a local pairwise CRF and use graph-cuts for discrete inference; as such their results can be limited by errors in superpixel computations, while ignoring  long-range superpixel dependencies. Our approach instead treats every pixel as a CRF node, exploits long-range dependencies, and uses  CRF inference to directly optimize a DCNN-driven cost function. 

% used in our fully-connected CRF. 
 %extract features via DCNN
%the one is by , who
% also employ CRF to propose diverse region proposals , and \citet{chen2014learning} which efficiently blend inference and learning to jointly train DCNN and CRF. 
%Our current model employs piecewise training, and jointly training is our next goal.


%{\bf{Conditional Random Fields for segmentation
%,  or crop several  bounding boxes from an image \citep{, girshick2014rcnn} to deal with multiple objects.


%: %}} Many semantic segmentation methods rely on Conditional Random Fields (CRFs), which

%{\bf{Deep Convolutional Neural Network for segmentation: }} Most of the systems built on top of DCNNs classify either a single object label for an entire image \citep{KrizhevskyNIPS2013, simonyan2014very, szegedy2014going}, or several object labels for bounding boxes within an image \citep{papandreou2014untangling, girshick2014rcnn}. Recently, there are several works that attemp to semantically segment an image with DCNNs. 



