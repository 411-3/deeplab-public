\section{Introduction}
\label{sec:intro}
%established in machine learning 
Deep Convolutional Neural Networks (DCNNs) had been the method of choice for document recognition since the work of \citet{LeCun1998}, but 
it is only recently that they have become the mainstream of high-level vison research.
Over the past two years  DCNNs have pushed the envelope of high-level vision system performance on several computer vision tasks, including image classifcation \citet{KrizhevskyNIPS2013, papandreou2014untangling, sermanet2013overfeat, simonyan2014very, szegedy2014going}, object detection \citet{girshick2014rcnn}, fine-graind categorization \citet{zhang2014part}, pose estimation \citet{chen2014articulated, tompson2014joint}, among others.
The most consistent results in these works is that DCNNs trained in an end-to-end manner  deliver  strikingly better results than systems relying on engineered features, such as SIFT or HOG features.

Still, the applications of DCNNs to low-level vision tasks, such as segmentation, or semantic image labelling  lags a bit behind in performance, with existing DCNN systems still underpeforming  state-of-the-art  computer vision systems using flat classifiers with engineered features, as witnessed e.g. in the comparisons of \cite{williams14} to the system of \cite{DollarZ13} on the task of boundary detection. 
 This can be partially attributed to the built-in  invariance of DCNNs to local image transformations, which underpins their ability to learn hierarchical abstractions of data \citep{zeiler2014visualizing}.
While this invariance is a blessing for high-level vision tasks, it can hamper low-level tasks, such as semantic segmentation - where we want precise localization, rather than abstraction of spatial details.  As such, DCNNs are not yet exploitable `out of the box' for labelling tasks. 

There are two technical hurdles in the application of DCNNs to image labelling tasks: the first problem relates to the reduction of signal resolution incurred by the repeated combination of max-pooling and downsampling (`striding') performed at every layer of DCNNs; DCNNs effectively downsample an $N\times N$ image by a factor of $N$ to reach a 
a single  classification decision for the whole image. This is clearly not appropriate for the task of pixel-level labelling, where $N \times N$ labels are needed. This could  be naively alleaviated by upsampling the image by a factor of $N$, yielding an $N^2\times N^2$ image, and an $N\times N$ set of labels. Instead of this naive approach we introducing into the problem the `atrous' (with holes) algorithm developped for  discrete wavelet analysis in \cite{}, resulting in substantial gains in efficiency. 



, and repeating the classification process multiple times - or making the last, fully connected layers of a DCNN convolutional, with a trivial $1\times 1 convolution kernel. Rather than using 

 this is clearly 

with the goal of arriving at 


 The second side relates to the invariance that is inherently necessary in order to obtain high-level, object-centric decisions. Even if the first aspect were resolved (e.g. naively 

In brief, we treat the first side of the problem by , and the second by coupling DCNNs with Conditional Random Field inference, so as to combine the high-level decisons of DCNNs with the sharp spatial localization of CRFs.

Our work draws inspiration from a broad range of works developped around the problem of semantic segmentation. We can clump 

 with the intent of reducing the set of positions entertained in higher-level classification tasks. 
\subsection{Related Work}
Our model is mostly related to two fields, and the goal of this work is to combine the best from them.

{\bf{Conditional Random Fields for segmentation: }} Many semantic segmentation methods rely on Conditional Random Fields (CRFs), which model the local interactions (via the pairwise potential) between pixels \citep{rother2004grabcut, shotton2009textonboost} or superpixels \citep{lucchi2011spatial}. Several works have been proposed to model the hierarchical dependency \citep{he2004multiscale, ladicky2009associative, lempitsky2011pylon} and the high-order potential (not just pairwise potential) \citep{delong2012fast, gonfaus2010harmony, kohli2009robust, krahenbuhl2011efficient}. Our model takes use of the fully connected CRF proposed by \citet{krahenbuhl2011efficient} for its efficient computation, and powerful long range dependency.

{\bf{Deep Convolutional Neural Network for segmentation: }} Most of the systems built on top of DCNNs classify either a single object label for an entire image \citep{KrizhevskyNIPS2013, simonyan2014very, szegedy2014going}, or several object labels for bounding boxes within an image \citep{papandreou2014untangling, girshick2014rcnn}. Recently, there are several works that attemp to semantically segment an image with DCNNs. \citet{girshick2014rcnn, hariharan2014simultaneous} take both bounding box proposals and masked regions as input to DCNN. The masked regions provide extra object shape informantion to the neural networks. These methods heavily depend on the region proposals \citep{arbelaez2014multiscale, Uijlings13}. Note that second order pooling \citep{carreira2012semantic} also assigns labels to regions proposals \citep{carreira2012cpmc}; however, hand-crafted features are employed. \citet{farabet2013learning} apply DCNNs to multi-resolution of the input image and employ a segmentation tree to smooth the prediction results. We also notice that there are serveral concurrent works, which bear similarities to our proposed model. \citet{mostajabi2014feedforward} propose to combine the multi-scale cues extracted by DCNNs (and some hand-crafted features) to label one superpixel. However, they results depend on the superpixels, which may leak out the object boundaries. \citet{hariharan2014hypercolumns} propose to concatenate the computed inter-mediate feature maps within the DCNNs for pixel classfication, and \citet{dai2014convolutional} propose to pool the inter-mediate feature maps by region proposals. Their models are two-step models, which depends on the accuracy of first step (\ie region proposals). On the other hand, our model is most similar to \citet{long2014fully, eigen2014predicting}, which directly apply DCNNs to the whole image in sliding window fashion. The last fully connected layers within a DCNN are replaced by convolutional layers. \citet{long2014fully} upsample and concatenate the scores from inter-mediate feature maps, while \citet{eigen2014predicting} refine the prediction result from coarse to fine by propagating the coarse results to another DCNN. Another recent work that also attempts to combine the effectivenss of graphical model and DCNN for segmentation is \citet{cogswell2014combining}, where the authors employ CRFs to propose diverse region proposals, an extension of \citet{yadollahpour2013discriminative}.


